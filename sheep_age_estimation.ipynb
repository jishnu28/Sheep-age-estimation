{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sheep Age Estimation Notebook\n",
        "\n",
        "### Introduction:\n",
        "This notebook covers the creation of a simple application that helps estimate the age of a sheep using just an image of the sheep.\\\n",
        "We use the following plug-in components:\n",
        "- LLM using ai-mixtral-8x7b-instruct\n",
        "- Fuyu-8b model from NVIDIA AI Catalog for image analysis and visual reasoning\n",
        "- Gradio for a simple User Interface to upload images\n",
        "\n",
        "![UI Example](assets/ui-example.png \"UI Example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DB2_ZSJVR3k",
        "outputId": "469cb408-18fa-4593-914d-384dbbe55bd8"
      },
      "outputs": [],
      "source": [
        "%pip install fastapi==0.104.1 uvicorn[standard]==0.24.0 python-multipart==0.0.6 langchain==0.1.9 unstructured[all-docs]==0.11.2 sentence-transformers==2.2.2 llama-index==0.9.22 dataclass-wizard==0.22.2 opencv-python==4.8.0.74 llama-hub==0.0.43 pymilvus==2.3.1 jupyterlab==4.0.8 langchain-core==0.1.29 langchain-nvidia-ai-endpoints==0.1.1 atlassian-python-api==3.41.4 gradio==3.48.0 markdownify==0.12.1 scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Export the NVIDIA_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAafaGASV0i0",
        "outputId": "780d4c7b-0a4c-41f0-8fc4-9cf96249df24"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
        "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
        "else:\n",
        "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
        "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
        "global nvapi_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Set 'ai-mixtral_8x7b-instruct' model as LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MTLN-nFZody"
      },
      "outputs": [],
      "source": [
        "#Set up Prerequisites for Image Captioning App User Interface\n",
        "import os\n",
        "import io\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "import gradio as gr\n",
        "import openai, httpx, sys\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlxAkmB_3MSK",
        "outputId": "24ca6f3a-e12f-4ad1-9eee-b448b9a90044"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "llm = ChatNVIDIA(model=\"ai-mixtral-8x7b-instruct\", nvidia_api_key=nvapi_key, max_tokens=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Wrap 'fuyu-8b' model in a tool for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkBiOkDNZsCN"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import BaseTool\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
        "import torch\n",
        "from tempfile import NamedTemporaryFile\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "def fetch_outputs(output):\n",
        "    collect_streaming_outputs=[]\n",
        "    for o in output:\n",
        "        try:\n",
        "            start = o.index('{')\n",
        "            jsonString=o[start:]\n",
        "            d = json.loads(jsonString)\n",
        "            temp=d['choices'][0]['delta']['content']\n",
        "            collect_streaming_outputs.append(temp)\n",
        "        except:\n",
        "            pass\n",
        "    outputs=''.join(collect_streaming_outputs)\n",
        "    return outputs.replace('\\\\','').replace('\\'','')\n",
        "\n",
        "def img2base64_string(img_path):\n",
        "    image = Image.open(img_path)\n",
        "    if image.width > 800 or image.height > 800:\n",
        "        image.thumbnail((800, 800))\n",
        "    buffered = io.BytesIO()\n",
        "    image.convert(\"RGB\").save(buffered, format=\"JPEG\", quality=85)\n",
        "    image_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "    return image_base64\n",
        "\n",
        "\n",
        "class ImageCaptionTool(BaseTool):\n",
        "    name = \"Image captioner from Fuyu\"\n",
        "    description = \"Use this tool when given the path to an image that you would like to be described. \" \\\n",
        "                  \"It will return a simple caption describing the image.\"\n",
        "\n",
        "    def _run(self, img_path):\n",
        "        print(\"run function in ImageCaptionTool called\")\n",
        "        invoke_url = \"https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b\"\n",
        "        stream = True\n",
        "\n",
        "\n",
        "        image_b64=img2base64_string(img_path)\n",
        "\n",
        "\n",
        "        assert len(image_b64) < 200_000, \\\n",
        "          \"To upload larger images, use the assets API (see docs)\"\n",
        "        headers = {\n",
        "          \"Authorization\": f\"Bearer {nvapi_key}\",\n",
        "          \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "          \"messages\": [\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f'Estimate the age of the sheep in this image in years, return a numerical estimate and include some reasoning for this estimate in your response. The response should be in the following format: \"Age: <numerical estimate> years. Reasoning: <reasoning>\". <img src=\"data:image/png;base64,{image_b64}\" />'\n",
        "            }\n",
        "          ],\n",
        "          \"max_tokens\": 1024,\n",
        "          \"temperature\": 0.20,\n",
        "          \"top_p\": 0.70,\n",
        "          \"seed\": 0,\n",
        "          \"stream\": stream\n",
        "        }\n",
        "\n",
        "        response = requests.post(invoke_url, headers=headers, json=payload)\n",
        "\n",
        "        if stream:\n",
        "            output=[]\n",
        "            for line in response.iter_lines():\n",
        "                if line:\n",
        "                    output.append(line.decode(\"utf-8\"))\n",
        "        else:\n",
        "            output=response.json()\n",
        "        out=fetch_outputs(output)\n",
        "        print(out)\n",
        "        return out\n",
        "\n",
        "    def _arun(self, query: str):\n",
        "        raise NotImplementedError(\"This tool does not support async\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Initialize our LangChain agent with the ImageCaptionTool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Y_qgJLZxCQ",
        "outputId": "ef0e3522-ef6c-44da-b52b-20033b1ed14f"
      },
      "outputs": [],
      "source": [
        "#initialize the agent\n",
        "tools = [ImageCaptionTool()]\n",
        "\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    max_iterations=5,\n",
        "    verbose=True,\n",
        "    memory=conversational_memory,\n",
        "    handle_parsing_errors=True,\n",
        "    early_stopping_method='generate'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcura8G2Z6P9"
      },
      "outputs": [],
      "source": [
        "def my_agent(img_path):\n",
        "    response = agent.invoke({\"input\":f'this is the image path: {img_path}'})\n",
        "    return response['output']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Wrap the agent in a Gradio UI for easy interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "QzZJ8guEZ8iF",
        "outputId": "a1bf9750-2da9-46dd-80fd-9016aae875de"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "ImageCaptionApp = gr.Interface(fn=my_agent,\n",
        "                    inputs=[gr.Image(label=\"Upload image\", type=\"filepath\")],\n",
        "                    outputs=[gr.Textbox(label=\"Caption\")],\n",
        "                    title=\"Sheep age estimation using Nvidia API\",\n",
        "                    description=\"Upload an image of a sheep to get an estimate for its age\",\n",
        "                    debug=True,\n",
        "                    allow_flagging=\"never\")\n",
        "\n",
        "ImageCaptionApp.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
